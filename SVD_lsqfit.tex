\documentclass[a4paper]{article}
% General document formatting
\usepackage[margin=0.7in]{geometry}
\usepackage[parfill]{parskip}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{empheq,etoolbox}
\usepackage{xcolor}


% Update display of subequation numbering (Xy) > (X.y)
\patchcmd{\subequations}% <cmd>
{\theparentequation\alph{equation}}% <search>
{\theparentequation.\alph{equation}}% <replace>
{}{}% <success><failure>



\begin{document}
The matrix $X \in \mathbb{R}^{n\times m}$ contains the surface elevation of $n$ grid cells (rows) for $m$ different training geometries (columns) representing the modelling output at several time steps and from different ensemble runs.
\begin{equation}
    X =
    \begin{bmatrix}
        x_{1\,(M_1)} & x_{1\,(M_2)} & ... & x_{1\,(M_m)}\\
        x_{2\,(M_1)} & x_{2\,(M_2)} & ... & x_{2\,(M_m)}\\
        ...          & ...          & ... & ...     \\
        x_{n\,(M_1)} & x_{n\,(M_2)} & ... & x_{n\,(M_m)}
    \end{bmatrix}
\end{equation}
where $M_i$ is the $i^{th}$ dataset generated by the model.
We want to find the vector $\mathbf{x}_\mathrm{rec}$ of reconstructed surface elevation data at a certain point in time, of which we only have incomplete knowledge with most of the values unknown
\begin{equation}
    \mathbf{x}_\mathrm{rec} =
    \begin{bmatrix}
        x_1\\
        x_2\\
        ...\\
        x_n       
    \end{bmatrix}_{\mathrm{rec}}
\end{equation}
Take the SVD of $X$:
\begin{equation}
    X = U \Sigma V^T
\end{equation}
For a full economy SVD, $U \in \mathbb{R}^{n\times m}$ represents the 'eigenglaciers' of each of the $m$ modes, $\Sigma \in \mathbb{R}^{m\times m}$ is a diagonal matrix of the corresponding singular values, and $V \in \mathbb{R}^{m\times m}$ is a collection of vectors that, for each of the $m$ modelling datasets, assign coefficients to each of the $m$ eigenglacier modes. Thus, if $\mathbf{x}_\mathrm{rec}$ is in the 'hull' of $X$, we can find coefficients comprising a vector $\mathbf{v}_\mathrm{rec}$ such that
\begin{equation}
    \mathbf{x}_\mathrm{rec} = U \Sigma \mathbf{v}_\mathrm{rec}
\end{equation}
% Since we only know $\mathbf{x}_\mathrm{rec}$ at certain indices, we instead require
% \begin{equation}
%     \mathbf{x}_\mathrm{data} = [U \Sigma]_\mathrm{data} \mathbf{v}_\mathrm{rec}
% \end{equation}
% where $\mathbf{x}_\mathrm{data}$ is a vector containing all known elements of $\mathbf{x}_\mathrm{rec}$ and $[U \Sigma]_\mathrm{data}$ is a matrix containing the corresponding rows of $U \Sigma$. The number of known data points, i.e. the length of $\mathbf{x}_\mathrm{data}$, is in the order of $\sim 10^5$ and therefore much larger than the number of model datasets $m$.
It is thus an overdetermined system for which there is no unique solution, but we can describe it as an optimization problem of least squares as
% \begin{equation} \label{eq:min_prob}
%     \min_{\mathbf{v_\mathrm{rec}}}    ||\mathbf{x}_\mathrm{data}- A \mathbf{v}_\mathrm{rec}||_2^2 + \lambda ||\mathbf{v}_\mathrm{rec}||_2^2
% \end{equation}
\begin{equation} \label{eq:min_prob}
    \min_{\mathbf{v}_\mathrm{rec}}    ||\mathbf{x}_\mathrm{rec}- A \mathbf{v}_\mathrm{rec}||_2^2 + \lambda ||\mathbf{v}_\mathrm{rec}||_2^2
\end{equation}
where $A=[U \Sigma]_\mathrm{data}$. Taking the gradient of Eq.~\eqref{eq:min_prob} and setting it to zero results in the normal equation
\begin{equation} \label{eq:normal_eq}
    (A^TA+\lambda I)\hat{\mathbf{v}}_\mathrm{rec} = A^T \mathbf{x}_\mathrm{rec}
\end{equation}
Taking the SVD of A again:
\begin{equation}
    A = U_A \Sigma_A V_A^T
\end{equation}
and inserting it into \eqref{eq:normal_eq} gives
\begin{align}
    (V_A\Sigma_A^T U_A^T U_A \Sigma_A V_A^T + \lambda I)\hat{\mathbf{v}}_\mathrm{rec} &= V_A\Sigma_A^T U_A^T \mathbf{x}_\mathrm{rec}\\
    \label{eq:unitary}
    (V_A\Sigma_A^T \Sigma_A V_A^T + V_A \lambda I V_A^T)\hat{\mathbf{v}}_\mathrm{rec} &= V_A\Sigma_A^T U_A^T \mathbf{x}_\mathrm{rec}\\
    V_A(\Sigma_A^T \Sigma_A + \lambda I) V_A^T \hat{\mathbf{v}}_\mathrm{rec} &= V_A\Sigma_A^T U_A^T \mathbf{x}_\mathrm{rec}\\
    % (\Sigma^T \Sigma + \lambda I)\hat{\mathbf{v}}_\mathrm{rec} &= \Sigma^T U^T \mathbf{x}_\mathrm{rec}
\end{align}
with $z = V_A^T \hat{\mathbf{v}}_\mathrm{rec}$ we get
\begin{equation}
    (\Sigma_A^T \Sigma_A + \lambda I) \mathbf{z} = \Sigma_A^T U_A^T \mathbf{x}_\mathrm{data}
\end{equation}



where $(\Sigma_A^T \Sigma_A + \lambda I)$ is a diagonal matrix. The solution of the system is thus
\begin{equation}
    \mathbf{z} = (\Sigma_A^T \Sigma_A + \lambda I)^{-1} \Sigma_A^T U_A^T \mathbf{x}_\mathrm{data}~.
\end{equation}
$\hat{\mathbf{v}}_\mathrm{rec} = V_A \mathrm{z}$ so 
\begin{equation}
    \hat{\mathbf{v}}_\mathrm{rec} = V_A (\Sigma^T \Sigma + \lambda I)^{-1} \Sigma^T U^T \mathbf{x}_\mathrm{data}~.
\end{equation}

For Eq.~\eqref{eq:unitary} to hold, $U$ must be unitary, i.e. $U^TU = U U^T = I$, which is not given for a truncated SVD (resulting in less columns of $U$) or when parts of the rows of $U$ are removed corresponding to grid cells where there is no data in $\mathbf{x}_\mathrm{rec}$. This is the reason for performing a second SVD on $A$.



\end{document}